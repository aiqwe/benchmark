{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82e23c2d-8f83-4472-80b8-de048ce4664d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T16:09:20.743242Z",
     "iopub.status.busy": "2024-09-13T16:09:20.743073Z",
     "iopub.status.idle": "2024-09-13T16:09:21.010200Z",
     "shell.execute_reply": "2024-09-13T16:09:21.009939Z",
     "shell.execute_reply.started": "2024-09-13T16:09:20.743220Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gpt import chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3c84c8c-d961-4e51-b786-87eba3ce9331",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T16:09:21.011170Z",
     "iopub.status.busy": "2024-09-13T16:09:21.011057Z",
     "iopub.status.idle": "2024-09-13T16:09:58.867123Z",
     "shell.execute_reply": "2024-09-13T16:09:58.865208Z",
     "shell.execute_reply.started": "2024-09-13T16:09:21.011161Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = chat(\"\"\"\n",
    "These filtering criteria result in 78 clean multiple-choice or exact-match tasks. Of these 78 tasks, the best\n",
    "reported model outperforms the average human-rater score on 42 of them, while no previous model surpasses the\n",
    "average human-rater score for the remaining 36. Based on manual inspection of 36 these tasks, we divide them\n",
    "into two categories. First, 13 tasks are extremely difficult for authors of this paper; they require domain-specific\n",
    "knowledge or are not practically solvable within twenty minutes. For example, Checkmate in One is not doable\n",
    "for non-chess players and requires tracking the state of the board across a long sequence of moves; Real or Fake\n",
    "Text is too challenging due to extremely long inputs; Moral Permissibility has an ambiguous task formulation\n",
    "(and we did not want to delegate agency to models about moral judgement scenarios (Talat et al., 2022)). We do\n",
    "not think these tasks can be attempted with CoT prompting, and we leave them as future work for more powerful\n",
    "models or prompting methods (refer to Appendix D for the exact list).\n",
    "We use the remaining 23 tasks as our curated benchmark that we refer to as BIG-Bench Hard (BBH). Two\n",
    "tasks in this benchmark (namely, Logical Deduction and Tracking Shuffled Objects) have three subtasks each.\n",
    "For all but three tasks in BBH, we take a random subset of 250 evaluation examples.2 Hence, there are 6,511\n",
    "evaluation examples in the benchmark in total. Assuming an average prompt length of 1.5K (this would depend\n",
    "on the length of the prompts), we find that evaluating BBH on text-davinci-002 would cost $195.33 USD at the\n",
    "current OpenAI API price of $0.02 USD per 1K tokens.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c12dc99e-e7af-428f-a871-b0385f62ad1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T16:09:58.869973Z",
     "iopub.status.busy": "2024-09-13T16:09:58.869469Z",
     "iopub.status.idle": "2024-09-13T16:09:58.882049Z",
     "shell.execute_reply": "2024-09-13T16:09:58.881439Z",
     "shell.execute_reply.started": "2024-09-13T16:09:58.869935Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "이러한 필터링 기준은 78개의 깨끗한 객관식 또는 정확히 일치하는 과제를 생성합니다.\\n  \n",
       "이 78개의 과제 중에서, 보고된 최고의 모델은 그중 42개의 과제에서 평균 인간 평가자 점수보다 뛰어난 성능을 보였으며, 나머지 36개의 과제에서는 이전의 어떤 모델도 평균 인간 평가자 점수를 넘지 못했습니다.\\n  \n",
       "이 36개의 과제에 대한 수동 검사를 바탕으로, 우리는 그것들을 두 가지 범주로 나눕니다.\\n  \n",
       "첫째, 13개의 과제는 이 논문의 저자들에게도 매우 어렵습니다; 이들은 도메인 특화된 지식을 요구하거나 20분 내에 실제로 해결할 수 없습니다.\\n  \n",
       "예를 들어, **한 수로 체크메이트 하기**는 체스 플레이어가 아닌 사람들에게는 불가능하며, 긴 수순의 움직임을 통해 보드의 상태를 추적해야 합니다;\\n  \n",
       "**실제 또는 가짜 텍스트**는 매우 긴 입력으로 인해 너무 어렵습니다;\\n  \n",
       "**도덕적 허용성**은 모호한 과제 형태를 가지고 있습니다(그리고 우리는 도덕적 판단 시나리오에 대해 모델에게 권한을 위임하고 싶지 않았습니다(Talat 외, 2022)).\\n  \n",
       "우리는 이러한 과제들이 CoT 프롬프트로 시도될 수 있다고 생각하지 않으며, 보다 강력한 모델이나 프롬프트 방법에 대한 향후 연구로 남겨둡니다(정확한 목록은 부록 D를 참조하십시오).\\n  \n",
       "\n",
       "우리는 남은 23개의 과제를 우리의 선별된 벤치마크인 **BIG-Bench Hard**(BBH)로 사용합니다.\\n  \n",
       "이 벤치마크의 두 가지 과제(즉, **논리적 추론**과 **셔플된 객체 추적**)는 각각 세 개의 하위 과제를 가지고 있습니다.\\n  \n",
       "BBH의 세 가지 과제를 제외한 모든 과제에 대해, 우리는 250개의 평가 예제를 무작위로 선택합니다.\\n  \n",
       "따라서, 이 벤치마크에는 총 6,511개의 평가 예제가 있습니다.\\n  \n",
       "평균 프롬프트 길이를 1.5K로 가정하면(이는 프롬프트의 길이에 따라 달라집니다), 우리는 **text-davinci-002**에서 BBH를 평가하는 데 현재 OpenAI API 가격인 1K 토큰당 $0.02 USD로 $195.33 USD의 비용이 들 것임을 발견했습니다.\\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9d7a615-e673-4654-af8d-b150b6b6d928",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T05:49:46.867926Z",
     "iopub.status.busy": "2024-09-14T05:49:46.867250Z",
     "iopub.status.idle": "2024-09-14T05:49:47.429725Z",
     "shell.execute_reply": "2024-09-14T05:49:47.429453Z",
     "shell.execute_reply.started": "2024-09-14T05:49:46.867870Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from benchmark import GithubReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d662342-0aa4-4b9d-90ab-e8314df1e48c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T05:49:47.430520Z",
     "iopub.status.busy": "2024-09-14T05:49:47.430382Z",
     "iopub.status.idle": "2024-09-14T05:49:49.383114Z",
     "shell.execute_reply": "2024-09-14T05:49:49.381978Z",
     "shell.execute_reply.started": "2024-09-14T05:49:47.430511Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reader = GithubReader(\"bigbenchhard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65ab2f18-d223-4851-b022-5fabbc96e829",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T05:49:49.386450Z",
     "iopub.status.busy": "2024-09-14T05:49:49.385937Z",
     "iopub.status.idle": "2024-09-14T05:49:49.807245Z",
     "shell.execute_reply": "2024-09-14T05:49:49.806519Z",
     "shell.execute_reply.started": "2024-09-14T05:49:49.386417Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boolean_expressions',\n",
       " 'causal_judgement',\n",
       " 'date_understanding',\n",
       " 'disambiguation_qa',\n",
       " 'dyck_languages',\n",
       " 'formal_fallacies',\n",
       " 'geometric_shapes',\n",
       " 'hyperbaton',\n",
       " 'logical_deduction_five_objects',\n",
       " 'logical_deduction_seven_objects',\n",
       " 'logical_deduction_three_objects',\n",
       " 'movie_recommendation',\n",
       " 'multistep_arithmetic_two',\n",
       " 'navigate',\n",
       " 'object_counting',\n",
       " 'penguins_in_a_table',\n",
       " 'reasoning_about_colored_objects',\n",
       " 'ruin_names',\n",
       " 'salient_translation_error_detection',\n",
       " 'snarks',\n",
       " 'sports_understanding',\n",
       " 'temporal_sequences',\n",
       " 'tracking_shuffled_objects_five_objects',\n",
       " 'tracking_shuffled_objects_seven_objects',\n",
       " 'tracking_shuffled_objects_three_objects',\n",
       " 'web_of_lies',\n",
       " 'word_sorting']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.get_files(\"cot-prompts\", pattern = \"*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5563c370-38db-41cb-aad1-b03742a802df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T05:50:44.227807Z",
     "iopub.status.busy": "2024-09-14T05:50:44.227204Z",
     "iopub.status.idle": "2024-09-14T05:50:44.235921Z",
     "shell.execute_reply": "2024-09-14T05:50:44.235009Z",
     "shell.execute_reply.started": "2024-09-14T05:50:44.227758Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bigbenchhard'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.benchmark_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff7ef2ca-3314-460b-8dee-a21c7cf6762b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T05:53:16.840035Z",
     "iopub.status.busy": "2024-09-14T05:53:16.839280Z",
     "iopub.status.idle": "2024-09-14T05:53:20.574110Z",
     "shell.execute_reply": "2024-09-14T05:53:20.573631Z",
     "shell.execute_reply.started": "2024-09-14T05:53:16.839970Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './tasks/bigbenchhard/cot-prompts/boolean_expressions.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m reader\u001b[38;5;241m.\u001b[39mget_files(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcot-prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m, pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m     data \u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mread_text(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcot-prompts/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;41mopen\u001b[39;49m\u001b[41m(\u001b[49m\u001b[38;5;124;41mf\u001b[39;49m\u001b[38;5;124;41m\"\u001b[39;49m\u001b[38;5;124;41m./tasks/\u001b[39;49m\u001b[38;5;132;41;01m{\u001b[39;49;00m\u001b[41mreader\u001b[49m\u001b[38;5;241;41m.\u001b[39;49m\u001b[41mbenchmark_name\u001b[49m\u001b[38;5;132;41;01m}\u001b[39;49;00m\u001b[38;5;124;41m/cot-prompts/\u001b[39;49m\u001b[38;5;132;41;01m{\u001b[39;49;00m\u001b[41mfname\u001b[49m\u001b[38;5;132;41;01m}\u001b[39;49;00m\u001b[38;5;124;41m.txt\u001b[39;49m\u001b[38;5;124;41m\"\u001b[39;49m\u001b[41m,\u001b[49m\u001b[41m \u001b[49m\u001b[38;5;124;41m\"\u001b[39;49m\u001b[38;5;124;41mw\u001b[39;49m\u001b[38;5;124;41m\"\u001b[39;49m\u001b[41m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(data)\n",
      "File \u001b[0;32m~/python_env/.base/lib/python3.10/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[41mio_open\u001b[49m\u001b[41m(\u001b[49m\u001b[41mfile\u001b[49m\u001b[41m,\u001b[49m\u001b[41m \u001b[49m\u001b[38;5;241;41m*\u001b[39;49m\u001b[41margs\u001b[49m\u001b[41m,\u001b[49m\u001b[41m \u001b[49m\u001b[38;5;241;41m*\u001b[39;49m\u001b[38;5;241;41m*\u001b[39;49m\u001b[41mkwargs\u001b[49m\u001b[41m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './tasks/bigbenchhard/cot-prompts/boolean_expressions.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for fname in reader.get_files(\"cot-prompts\", pattern = \"*.txt\"):\n",
    "    data = reader.fs.read_text(f\"cot-prompts/{fname}.txt\")\n",
    "    folder = f\"./tasks/{reader.benchmark_name}/cot-prompts\n",
    "    if not os.exists(folder):\n",
    "        os.makedirs()\n",
    "    with open(f\"./tasks/{reader.benchmark_name}/cot-prompts/{fname}.txt\", \"w\") as f:\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742a9d99-c428-4eee-ad6c-2d9f5115c8e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
